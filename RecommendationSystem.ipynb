{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvN-J2X6eB0F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'FINAL jester 2006-15.csv'  # Adjusted to correct file naming\n",
        "jokes_path = 'Dataset3JokeSet.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "jokes = pd.read_csv(jokes_path, header=None, names=[\"Joke_Text\"])\n",
        "\n",
        "# Replace 99 with NaN for unrated jokes\n",
        "data = data.replace(99, np.nan)\n",
        "\n",
        "# Separate user IDs and ratings\n",
        "data_users = data.iloc[:, 0]  # User joke count column\n",
        "data_ratings = data.iloc[:, 1:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "przI1sxieB0T"
      },
      "outputs": [],
      "source": [
        "# Filter active users (threshold = 80 rated jokes)\n",
        "threshold = 80\n",
        "active_users_mask = data_ratings.notna().sum(axis=1) >= threshold\n",
        "active_data = data_ratings[active_users_mask]\n",
        "active_users = data_users[active_users_mask]\n",
        "\n",
        "# Train-test split\n",
        "train_data, test_data = train_test_split(active_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Replace NaN with user-mean for better similarity computation\n",
        "user_means = train_data.mean(axis=1)\n",
        "train_filled = train_data.T.fillna(user_means).T\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity_matrix = cosine_similarity(train_filled)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQeFSdDmeB0p"
      },
      "outputs": [],
      "source": [
        "# Function to predict ratings based on K most similar users\n",
        "def predict_ratings(user_index, joke_index, k):\n",
        "    # Get similarity scores for the user\n",
        "    user_similarity = similarity_matrix[user_index]\n",
        "\n",
        "    # Sort and pick top K similar users (excluding the user itself)\n",
        "    similar_users = np.argsort(-user_similarity)[1 : k + 1]\n",
        "\n",
        "    # Gather ratings from similar users for the specific joke\n",
        "    ratings = train_data.iloc[similar_users, joke_index]\n",
        "    similarities = user_similarity[similar_users]\n",
        "\n",
        "    # Exclude NaN ratings\n",
        "    mask = ~ratings.isna()\n",
        "    ratings = ratings[mask]\n",
        "    similarities = similarities[mask]\n",
        "\n",
        "    if len(ratings) == 0:\n",
        "        return user_means.iloc[user_index]  # Default to user's mean rating\n",
        "\n",
        "    # Weighted average of ratings\n",
        "    prediction = np.dot(ratings, similarities) / np.sum(similarities)\n",
        "    return prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXGf30ldeB0r"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "def evaluate_model(k):\n",
        "    predictions, actuals = [], []\n",
        "    test_instances = []  # For storing test instances with original and predicted values\n",
        "\n",
        "    for user_index in range(test_data.shape[0]):\n",
        "        for joke_index in range(test_data.shape[1]):\n",
        "            actual = test_data.iloc[user_index, joke_index]\n",
        "\n",
        "            if not np.isnan(actual):\n",
        "                prediction = predict_ratings(user_index, joke_index, k)\n",
        "                if not np.isnan(prediction):\n",
        "                    predictions.append(prediction)\n",
        "                    actuals.append(actual)\n",
        "                    test_instances.append((user_index, joke_index, actual, prediction))\n",
        "\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    return mae, test_instances\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bJHVyyVeB0r"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Baseline random recommender system\n",
        "def random_recommender(k):\n",
        "    predictions, actuals = [], []\n",
        "    for user_index in range(test_data.shape[0]):\n",
        "        for joke_index in range(test_data.shape[1]):\n",
        "            actual = test_data.iloc[user_index, joke_index]\n",
        "\n",
        "            if not np.isnan(actual):\n",
        "                # Randomly select K users\n",
        "                random_users = train_data.iloc[:, joke_index].dropna().sample(min(k, len(train_data.iloc[:, joke_index].dropna())), replace=False)\n",
        "\n",
        "                if len(random_users) > 0:\n",
        "                    random_prediction = random_users.mean()\n",
        "                else:\n",
        "                    random_prediction = np.nan  # If no valid ratings, set to NaN\n",
        "\n",
        "                if not np.isnan(random_prediction):  # Only include valid predictions\n",
        "                    predictions.append(random_prediction)\n",
        "                    actuals.append(actual)\n",
        "\n",
        "    return mean_absolute_error(actuals, predictions) if predictions else np.nan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EJZqaBCeB0w"
      },
      "outputs": [],
      "source": [
        "# Recommend top N jokes for a user\n",
        "def recommend_jokes(user_index, k, top_n=5):\n",
        "    joke_predictions = []\n",
        "\n",
        "    for joke_index in range(train_data.shape[1]):\n",
        "        prediction = predict_ratings(user_index, joke_index, k)\n",
        "        if not np.isnan(prediction):\n",
        "            joke_predictions.append((joke_index, prediction))\n",
        "\n",
        "    # Sort by predicted rating and pick top N\n",
        "    joke_predictions = sorted(joke_predictions, key=lambda x: -x[1])[:top_n]\n",
        "    return [(jokes.iloc[joke_id].Joke_Text, score) for joke_id, score in joke_predictions]\n",
        "\n",
        "# Find top 2K users with similarity scores\n",
        "def get_top_similar_users(k, top_n=2000):\n",
        "    top_users = []\n",
        "\n",
        "    for user_index in range(similarity_matrix.shape[0]):\n",
        "        user_similarity = similarity_matrix[user_index]\n",
        "        similar_users = np.argsort(-user_similarity)[1 : k + 1]\n",
        "        scores = user_similarity[similar_users]\n",
        "        top_users.append((user_index, list(zip(similar_users, scores))))\n",
        "\n",
        "    return top_users[:top_n]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmS9WN8aeB00"
      },
      "outputs": [],
      "source": [
        "# Find optimal K\n",
        "k_values = [5, 10, 20, 50]\n",
        "model_results, baseline_mae = {}, {}\n",
        "\n",
        "for k in k_values:\n",
        "    mae, test_instances = evaluate_model(k)\n",
        "    model_results[k] = {\n",
        "        \"mae\": mae,\n",
        "        \"test_instances\": test_instances,\n",
        "    }\n",
        "    baseline_mae[k] = random_recommender(k)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASqkOYvEeB1d",
        "outputId": "4bc2da38-8de7-44e7-b464-3b470f98a3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K | Collaborative Filtering MAE | Random Recommender MAE\n",
            "5 | 5.7134 | 4.4255\n",
            "10 | 5.6879 | 4.2382\n",
            "20 | 5.6814 | 4.1436\n",
            "50 | 5.6627 | 4.0863\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "print(\"K | Collaborative Filtering MAE | Random Recommender MAE\")\n",
        "for k in k_values:\n",
        "    print(f\"{k} | {model_results[k]['mae']:.4f} | {baseline_mae[k]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJD1O43PeB1t",
        "outputId": "d6426a33-4c5d-4364-82a4-4b8b29985bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Data Instances (User Index, Joke Index, Original, Predicted):\n",
            "(0, 6, -8.34375, -4.154898970131304)\n",
            "(0, 7, -8.34375, -0.7968891917150512)\n",
            "(0, 12, -8.34375, -2.9924180330852805)\n",
            "(0, 14, -6.90625, -0.3690117676660045)\n",
            "(0, 15, 2.09375, 1.550551670673912)\n",
            "(0, 16, 1.375, 1.0385968636707699)\n",
            "(0, 17, -1.3125, 3.4644530107637705)\n",
            "(0, 18, 3.90625, 3.2710170216507444)\n",
            "(0, 20, 9.09375, 4.766570768147534)\n",
            "(0, 21, 6.96875, 4.004287699191459)\n"
          ]
        }
      ],
      "source": [
        "# Print test instances for the last evaluated K\n",
        "print(\"\\nTest Data Instances (User Index, Joke Index, Original, Predicted):\")\n",
        "for instance in model_results[k_values[-1]][\"test_instances\"][:10]:\n",
        "    print(instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc5vogcdeB1w",
        "outputId": "ea20c510-b47a-4b02-b3c1-256631c91ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top Recommended Jokes:\n",
            "Score: 7.27, Joke: A little boy goes to his dad and asks, \"What is politics?\"His dad says, \"Well son, let me try to explain it this way: I'm the breadwinner of the family, so let's call me capitalism. Your Mom, she's the administrator of the money, so we'll call her the government. We're here to take care of your needs, so we'll call you the people. The nanny, we'll consider her the working class. And your baby brother, we'll call him the future. Now, think about that and see if that makes sense.\" So the little boy goes off to bed thinking about what dad had said. Later that night, he hears his baby brother crying, so he gets up to check on him. He finds that the baby has severely soiled his diaper. So the little boy goes to his parents' room and finds his mother sound asleep. Not wanting to wake her, he goes to the nanny's room. Finding the door locked, he peeks in the keyhole and sees his father in bed with the nanny. He gives up and goes back to bed. The next morning, the little boy says to his father, \"Dad, I think I understand the concept of politics now.\" The father says, \"Good, son. Tell me in your own words what you think politics is all about.\" The little boy replies, \"Well, while capitalism is screwing the working class, the government is sound asleep, the people are being ignored and the future is in deep shit.\"\n",
            "Score: 7.14, Joke: The Pope dies and, naturally, goes to heaven. He's met by the reception committee, and after a whirlwind tour he is told that he can enjoy any of the myriad of recreations available. He decides that he wants to read all of the ancient original text of the Holy Scriptures, so he spends the next eon or so learning languages. After becoming a linguistic master, he sits down in the library and begins to pour over every version of the Bible, working back from most recent \"Easy Reading\" to the original script. All of a sudden there is a scream in the library. The Angels come running in only to find the Pope huddled in his chair, crying to himself and muttering, \"An 'R'! The scribes left out the 'R'.\"  A particularly concerned Angel takes him aside, offering comfort, asks him what the problem is and what does he mean.  After collecting his wits, the Pope sobs again, \"It's the letter 'R'. They left out the 'R'. The word was supposed to be CELEBRATE!\"\n",
            "Score: 6.55, Joke: A preist, a 12-year-old kid, and the smartest guy in the world are on a plane. The pilot screams, \"The plane is going down! You have to jump!\" He then grabs a parachute and jumps off, leaving only two more parachutes on the plane. The smartest guy in the world says, \"I have to go. I mean, I'm the smartest guy in the world!\" He grabs a parachute, and jumps. The priest then looks at the 12-year-old kid, and says, \"Go, my son. You have a long life to live.\" The kid calmly responds: \"Dude, chill. We'll be fine. The 'smartest guy in the world' took my backpack.\"\n",
            "Score: 6.55, Joke: This guys wife asks, \"Honey if I died would you remarry?\" and he replies, \"Well, after a considerable period of grieving, we all need companionship, I guess I would.\"  She then asks, \"If I died and you remarried, would she live in this house?\" and he replies, \"We've spent a lot of time and money getting this house just the way we want it. I'm not going to get rid of my house, I guess she would.\"  \"If I died and you remarried, and she lived in this house, would she sleep in our bed?\" and he says, \"That bed is brand new, we just paid two thousand dollars for it, it's going to last a long time, I guess she would.\"  So she asks, \"If I died and you remarried, and she lived in this house, and slept in our bed, would she use my golf clubs?\"  \"Oh no, she's left handed.\"\n",
            "Score: 6.40, Joke: Recently a teacher, a garbage collector, and a lawyer wound up together at the Pearly Gates. St. Peter informed them that in order to get into Heaven, they would each have to answer one question. St. Peter addressed the teacher and asked, \"What was the name of the ship that crashed into the iceberg? They just made a movie about it.\" The teacher answered quickly, \"That would be the Titanic.\" St. Peter let him through the gate. St. Peter turned to the garbage man and, figuring Heaven didn't really need all the odors that this guy would bring with him, decided to make the question a little harder: \"How many people died on the ship?\" Fortunately for him, the trash man had just seen the movie. \"1,228,\" he answered. \"That's right! You may enter.\" St. Peter turned to the lawyer: \"Name them.\"\n"
          ]
        }
      ],
      "source": [
        "# Recommend jokes for the first user in the test set\n",
        "user_index = 5  # Replace with the desired user index from test_data\n",
        "top_jokes = recommend_jokes(user_index, k=10, top_n=5)\n",
        "print(\"\\nTop Recommended Jokes:\")\n",
        "for joke, score in top_jokes:\n",
        "    print(f\"Score: {score:.2f}, Joke: {joke}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cAH06hIeB1x",
        "outputId": "48ad4c55-4603-4cde-b83c-675ead4d7355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 2K Users with Similarity Scores:\n",
            "User 0: [(598, 0.7780871229256645), (4440, 0.7332611382135881), (1644, 0.7255292209595938), (4119, 0.7220933686532582), (3109, 0.7207765247415778)]\n",
            "User 1: [(2055, 0.653968033432101), (4718, 0.6309917564068408), (1644, 0.6301929245939322), (3041, 0.6176487779412523), (4930, 0.6133382549870979)]\n",
            "User 2: [(2595, 0.3305844958932683), (2626, 0.31985771207293495), (4193, 0.3076461757332851), (1876, 0.292978694554563), (1908, 0.28612309715275197)]\n",
            "User 3: [(463, 0.6388375473114427), (1683, 0.6348254728787144), (1723, 0.6312273701845401), (3015, 0.6120379244273043), (2164, 0.6116405342951784)]\n",
            "User 4: [(3737, 0.8395978245391773), (3190, 0.8357058286316273), (2208, 0.8317445090771605), (3915, 0.8311641345712943), (522, 0.8309479832914495)]\n",
            "User 5: [(1898, 0.8071276236922876), (1668, 0.7805575422355802), (226, 0.7659493582923735), (4850, 0.7649993734598965), (969, 0.7607281597429998)]\n",
            "User 6: [(1179, 0.7986079435790834), (3197, 0.7628438725663185), (3027, 0.7616728178820948), (4035, 0.7608271490612606), (2936, 0.7607810868166247)]\n",
            "User 7: [(466, 0.7992322673311827), (1235, 0.7989387515287985), (1814, 0.7978204735895984), (3634, 0.7898619216206555), (968, 0.789210237611244)]\n",
            "User 8: [(33, 0.9499813804293293), (1642, 0.9489909419501192), (1949, 0.9487498419797005), (3147, 0.9481197153432999), (3604, 0.948062349221426)]\n",
            "User 9: [(1949, 0.986378930596652), (1498, 0.9787721364969342), (3795, 0.9785761678807511), (523, 0.9785272074434426), (4989, 0.9778892288817611)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Display top 2K users with similarity scores\n",
        "top_users = get_top_similar_users(k=10, top_n=2000)\n",
        "print(\"\\nTop 2K Users with Similarity Scores:\")\n",
        "for user_index, similarities in top_users[:10]:  # Display first 10 users\n",
        "    print(f\"User {user_index}: {similarities[:5]}\")  # Display top 5 similar users for brevity\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}